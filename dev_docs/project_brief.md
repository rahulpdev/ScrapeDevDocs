**Project Goal:**

To crawl online developer documentation specified in a tree-structure markdown file and maintain an up-to-date, structured set of offline markdown files containing the documentation content.

**Project Scope:**

1.  **URL Content Extraction and Markdown Conversion**

    - Develop a script to:
      - Crawl a list of URLs and output a markdown file for each website page.
      - For each retrieved page:
        - Preserve the heading hierarchy of the original content.
        - Do NOT remove any content between section headers.
        - Replace all relative path URLs with full path URLs.
        - Preserve all external links as markdown references.
        - Process all images referenced on the webpage:
          - Identify the image URL and alt text from the HTML.
          - If the image URL points to an SVG file (based on extension or content type):
            - Retrieve the SVG file.
            - Attempt conversion to a Mermaid diagram, preserving:
              - Node relationships and hierarchy
              - Connector directions and types
              - Label positioning and content
            - On any conversion failure or invalid SVG:
              - Preserve the original image reference using its URL and alt text.
              - Do not include error notices or retain failed conversion attempts.
            - Discard the SVG file after use.
          - For all other image formats (PNG, JPG, etc.) or if the format cannot be determined as SVG:
            - Do **not** download the image file.
            - Preserve the original image reference using its URL and alt text.
        - Preserve "Last Updated" data verbatim.
      - For each markdown file:
        - Add the webpage full path URL to the end of the file.
        - Check off the webpage URL and add a timestamp in `<website name>_scrape_checklist.md` after file completion.
        - Save the completed file in its `<website name>_docs` folder.
      - For each URL, maintain:
        - Single `<website name>_docs` folder (no sub folders) in project root
        - `<website name>_scrape_checklist.md` crawler task tracker in project root
        - Processed markdown files containing:
          - Original content structure
          - Converted mermaid diagrams (where applicable)
          - Source URL reference
      - Accept as a terminal input (with input validation) a URL to a GitHub repository raw file (or similar raw text source) that contains a tree structure markdown diagram (like the example provided: `https://raw.githubusercontent.com/rahulpdev/WebsiteNavbarMap/refs/heads/main/output_maps/developers_home_assistant_io_nav_map.md`). Each branch of the tree represents a URL to be crawled.
      - After accepting a valid terminal input, create/overwrite the content of `<website name>_scrape_checklist.md` with a checklist of the URLs extracted from the tree structure diagram.
      - During execution, display a progress bar in the terminal indicating the number of URLs successfully processed out of the total (e.g., "Processed Y of X URLs").

2.  **Architectural Assumptions**

    - **File Handling**:

      - Output files overwrite existing files by default
      - No versioning/history tracking required
      - Local filesystem is primary storage medium

    - **Interface Priorities**:

      - CLI as primary user interface for providing the input URL (pointing to the tree structure markdown file).
      - No GUI/web interface planned.

    - **Output Standards**:

      - Markdown as sole output format
      - Tables preferred over other data representations
      - SVG conversion focuses on structural accuracy

    - **Operational Constraints**:
      - No cloud storage integration
      - No authentication requirements
      - Limited error recovery capabilities

3.  **Automation**

    - No automated execution is currently planned. Execution is manual via CLI.

4.  **Output**

    - A public GitHub repository containing:
      - The scripts for the content extraction functions.
      - Folders containing the markdown files generated by the script (one folder per source website).
    - `.gitignore` file containing all usual exclusions and also:
      - `<website name>_scrape_checklist.md`: Each task tracker produced by the script.
      - Each folder containing the markdown files generated by the script (e.g., `<website name>_docs/`).

5.  **Error Handling**
    - Implement robust error handling for:
      1.  **Network Errors:**
          - Use `try-except` blocks to catch `requests.exceptions.RequestException` during URL fetching.
          - Log error messages, including the URL that caused the error.
          - Implement retry logic with exponential backoff for transient network errors.
          - If a link is broken, log the error, and continue to the next link.
      2.  **HTML Parsing Errors:**
          - Use `try-except` blocks to catch `BeautifulSoup` parsing errors.
          - Log error messages, including the URL and the parsing issue.
          - Use robust HTML selectors to minimize parsing failures.
      3.  **File I/O Errors:**
          - Use `try-except` blocks to catch file I/O errors (e.g., `IOError`).
          - Log error messages, including the file path and the error details.
          - Ensure proper file permissions and directory existence.
      4.  **Logging:**
          - Implement comprehensive logging to track the execution of the scripts.
          - Log informational messages, warnings, and errors.
          - Use a structured logging format (e.g., JSON) for easier analysis.
